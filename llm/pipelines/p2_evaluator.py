# llm/pipelines/p2_evaluator.py

import json
import torch
import re
import gc
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
import chromadb
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from core.config import settings # core/config.pyì—ì„œ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°

# --- P2ìš© í—¬í¼ í•¨ìˆ˜ ---
def load_json_file(filepath):
    """JSON íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        print(f"ğŸ“ JSON íŒŒì¼ ë¡œë”© ì‹œë„: {filepath}")
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(f"âœ… JSON íŒŒì¼ ë¡œë”© ì„±ê³µ: {filepath}")
            return data
    except FileNotFoundError:
        print(f"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {filepath} - {e}")
        return None

# --- P2 íŒŒì´í”„ë¼ì¸ì˜ êµ¬ì„± ìš”ì†Œë“¤ ---

class SimilarityEvaluator:
    """ë¬¸ì¥ ì„ë² ë”©ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ì „ë‹´í•˜ëŠ” í´ë˜ìŠ¤"""
    _instance = None
    _model = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(SimilarityEvaluator, cls).__new__(cls)
        return cls._instance

    def _load_model(self):
        if self._model is None:
            print("ğŸ”„ ì§ë¬´ ìœ ì‚¬ë„ í‰ê°€ë¥¼ ìœ„í•œ ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            self._model = SentenceTransformer(settings.EMBEDDING_MODEL)
            print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        self._load_model()
        embeddings = self._model.encode([text1, text2])
        return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]

class LLMManager:
    """P2 í‰ê°€ìš© LLM ëª¨ë¸ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ (dependencies.pyì—ì„œ ê³µìœ ë¨)"""
    def __init__(self, model_name):
        self.model_name = model_name
        self.model, self.tokenizer = None, None

    def load_model(self):
        if self.model is None or self.tokenizer is None:
            print(f"\n>>> Base ì–‘ìí™” LLM ëª¨ë¸({self.model_name})ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
            
            # Base ì–‘ìí™” ëª¨ë¸ ë¡œë”© ì„¤ì •
            model_kwargs = {
                "device_map": "auto",
                "trust_remote_code": True,
                "low_cpu_mem_usage": settings.LOW_CPU_MEM_USAGE
            }
            
            # ì–‘ìí™” ì„¤ì •ì— ë”°ë¼ dtype ê²°ì •
            if settings.USE_QUANTIZATION:
                if settings.QUANTIZATION_BITS == 16:
                    model_kwargs["torch_dtype"] = torch.bfloat16
                    print("ğŸ”§ Base ì–‘ìí™” ëª¨ë“œ: 16ë¹„íŠ¸ ì •ë°€ë„ (bfloat16)")
                elif settings.QUANTIZATION_BITS == 8:
                    model_kwargs["load_in_8bit"] = True
                    print("ğŸ”§ Base ì–‘ìí™” ëª¨ë“œ: 8ë¹„íŠ¸ ì–‘ìí™”")
                elif settings.QUANTIZATION_BITS == 4:
                    # 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì • (ìµœëŒ€ ì••ì¶•)
                    model_kwargs["load_in_4bit"] = True
                    model_kwargs["bnb_4bit_compute_dtype"] = getattr(torch, settings.BNB_4BIT_COMPUTE_DTYPE)
                    model_kwargs["bnb_4bit_use_double_quant"] = settings.BNB_4BIT_USE_DOUBLE_QUANT
                    model_kwargs["bnb_4bit_quant_type"] = settings.BNB_4BIT_QUANT_TYPE
                    print("ğŸ”§ Base ì–‘ìí™” ëª¨ë“œ: 4ë¹„íŠ¸ ì–‘ìí™” (ìµœëŒ€ ì••ì¶•, ë©”ëª¨ë¦¬ ~87% ì ˆì•½)")
                    print(f"   - ê³„ì‚° ì •ë°€ë„: {settings.BNB_4BIT_COMPUTE_DTYPE}")
                    print(f"   - ì´ì¤‘ ì–‘ìí™”: {settings.BNB_4BIT_USE_DOUBLE_QUANT}")
                    print(f"   - ì–‘ìí™” íƒ€ì…: {settings.BNB_4BIT_QUANT_TYPE}")
            else:
                model_kwargs["torch_dtype"] = torch.float16
                print("ğŸ”§ ì¼ë°˜ ëª¨ë“œ: 16ë¹„íŠ¸ ì •ë°€ë„ (float16)")
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, 
                padding_side='left',
                trust_remote_code=True
            )
            self.tokenizer.pad_token = self.tokenizer.eos_token
            print("âœ… Base ì–‘ìí™” LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

    def generate(self, prompt: str) -> str:
        if not self.model or not self.tokenizer:
            raise RuntimeError("LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        if 'token_type_ids' in inputs:
            del inputs['token_type_ids']
        output_tokens = self.model.generate(
            **inputs, max_new_tokens=512,
            eos_token_id=self.tokenizer.eos_token_id,
            do_sample=True, temperature=0.1, top_p=0.9
        )
        return self.tokenizer.decode(output_tokens[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()


class QuantitativeEvaluator:
    """scoring_rules.jsonì„ ê¸°ë°˜ìœ¼ë¡œ ì •ëŸ‰ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì—”ì§„"""
    def __init__(self, rules_path, universities_kb_path, certifications_kb_path, similarity_evaluator):
        self.rules_data = load_json_file(rules_path)
        self.uni_kb = {uni['name']: uni['group'] for uni in load_json_file(universities_kb_path)}
        self.cert_type_map = self._build_cert_type_map(load_json_file(certifications_kb_path))
        self.similarity_evaluator = similarity_evaluator
        self.target_job_role = self.rules_data.get('common_rules', {}).get('job_role', 'Software Engineer')
        if not self.rules_data:
            raise ValueError("Scoring rules íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def _build_cert_type_map(self, cert_data):
        type_map = {}
        for cert in cert_data:
            cert_type = cert.get('type')
            if cert_type:
                type_map[cert['name'].lower()] = cert_type  
                for alias in cert.get('aliases', []):
                    type_map[alias.lower()] = cert_type
        return type_map

    def _find_answer_item(self, rule_name, applicant_data):
        """Name ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€ í•­ëª©ì„ ì°¾ìŠµë‹ˆë‹¤."""
        for item in applicant_data.get('resumeItemAnswers', []):
            if item.get('resumeItemName') == rule_name:
                return item
        return {}

    def _evaluate_category(self, rule, applicant_answer_item):
        applicant_category = applicant_answer_item.get('selectedCategory')
        applicant_content = applicant_answer_item.get('resumeContent', '')

        if not applicant_category:
            if 'í•™ë ¥' in rule.get('name', ''):
                uni_name_match = re.search(r'(\w+ëŒ€í•™êµ|\w+ëŒ€)', applicant_content)
                uni_name = uni_name_match.group(1) if uni_name_match else None
                if uni_name and uni_name in self.uni_kb:
                    applicant_category = self.uni_kb[uni_name]
                else:
                    return 0
            else:
                for criterion in rule.get('criteria', []):
                    if criterion.get('description', '').strip() in applicant_content:
                        return criterion.get('score_per_grade', 0)
                return 0

        if applicant_category:
            for criterion in rule.get('criteria', []):
                criterion_desc = criterion.get('description', '')
                if applicant_category == criterion_desc or applicant_category in criterion_desc.split('Â·'):
                    return criterion.get('score_per_grade', 0)

        return 0

    def _evaluate_numeric_range(self, rule, applicant_answer_item):
        applicant_content = applicant_answer_item.get('resumeContent', '')
        gpa_match = re.search(r'(ì´ê³µ|ì¸ë¬¸)\s*(\d+(?:\.\d+)?)', applicant_content)
        if gpa_match:
            applicant_major, applicant_score = gpa_match.groups()
            applicant_score = float(applicant_score)
            for criterion in rule.get('criteria', []):
                desc = criterion.get('description', '')
                major_rule_match = re.search(rf'{applicant_major}\s*([<â‰¥>â‰¤])\s*(\d+(?:\.\d+)?)', desc)
                if major_rule_match:
                    operator, threshold = major_rule_match.groups()
                    threshold = float(threshold)
                    if (operator == 'â‰¥' and applicant_score >= threshold) or \
                       (operator == '<' and applicant_score < threshold):
                        return criterion.get('score_per_grade', 0)
            return 0
        return 0

    def _evaluate_hours_range(self, rule, applicant_answer_item):
        applicant_content = applicant_answer_item.get('resumeContent', '')
        simple_numeric_match = re.search(r'(\d+)', applicant_content)
        if not simple_numeric_match: return 0
        value = int(simple_numeric_match.group(1))

        sorted_criteria = sorted(rule.get('criteria', []), key=lambda x: int(x.get('description', 0)), reverse=True)
        for criterion in sorted_criteria:
            if value >= int(criterion.get('description', 0)):
                return criterion.get('score_per_grade', 0)
        return 0

    def _evaluate_duration_based(self, rule, applicant_answer_item):
        applicant_content = applicant_answer_item.get('resumeContent', '').strip()
        if not applicant_content: return 0
        duration_match = re.search(r'(\d+)\s*ê°œì›”', applicant_content)
        if not duration_match: return 0
        months = int(duration_match.group(1))
        sorted_criteria = sorted(rule.get('criteria', []), key=lambda x: int(x.get('description', 0)), reverse=True)
        for criterion in sorted_criteria:
            if months >= int(criterion.get('description', 0)):
                return criterion.get('score_per_grade', 0)
        return 0

    def _evaluate_rule_based_count(self, rule, applicant_answer_item):
        applicant_content = applicant_answer_item.get('resumeContent', '').lower().strip()
        applicant_certs = [c.strip() for c in applicant_content.split(',')] if applicant_content else []
        total_score = 0

        for sub_rule in rule.get('criteria', []):
            rule_cert_type = sub_rule.get('description', '')
            points_per_item = sub_rule.get('score_per_grade', 0)  # ê¸°ë³¸ê°’ 0ìœ¼ë¡œ ì„¤ì •
            max_items = sub_rule.get('max_items', float('inf'))

            count = 0
            for cert_name in applicant_certs:
                applicant_cert_type = self.cert_type_map.get(cert_name)
                if applicant_cert_type and applicant_cert_type in rule_cert_type:
                    count += 1

            score_for_rule = min(count, max_items) * points_per_item
            total_score += score_for_rule

        return min(total_score, rule.get('score_weight', total_score))

    def _evaluate_text_based(self, rule, applicant_answer_item):
        """TEXT, NUMBER, DATE, FILE, SELECT íƒ€ì…ì— ëŒ€í•œ ê¸°ë³¸ í‰ê°€"""
        applicant_content = applicant_answer_item.get('resumeContent', '').strip()
        if not applicant_content:
            return 0
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ìµœì†Œ ì ìˆ˜, í‰ê°€ ê¸°ì¤€ê³¼ ë§¤ì¹­ë˜ë©´ ë” ë†’ì€ ì ìˆ˜
        for criterion in rule.get('criteria', []):
            criterion_desc = criterion.get('description', '').strip()
            if criterion_desc and criterion_desc.lower() in applicant_content.lower():
                score = criterion.get('score_per_grade', 0)
                print(f"[DEBUG] í…ìŠ¤íŠ¸ ë§¤ì¹­ ì„±ê³µ: '{criterion_desc}' in '{applicant_content}' -> {score}ì ")
                return score
        
        # ë§¤ì¹­ë˜ëŠ” ê¸°ì¤€ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì ìˆ˜ (ë‚´ìš©ì´ ìˆê¸°ë§Œ í•˜ë©´)
        default_score = rule.get('criteria', [{}])[0].get('score_per_grade', 0) if rule.get('criteria') else 0
        print(f"[DEBUG] í…ìŠ¤íŠ¸ ê¸°ë³¸ ì ìˆ˜: '{applicant_content}' -> {default_score}ì ")
        return default_score

    def _evaluate_score_range(self, rule, applicant_answer_item):
        applicant_content = applicant_answer_item.get('resumeContent', '').strip().lower()
        if not applicant_content:
            return 0

        # ì§€ì›ì ì…ë ¥ì„ ê°œë³„ í† í°ìœ¼ë¡œ ë¶„ë¦¬ (ì˜ˆ: "opic ih" â†’ ["opic", "ih"])
        applicant_tokens = applicant_content.split()

        for criterion in rule.get('criteria', []):
            criterion_desc = criterion.get('description', '').lower()
            criterion_tokens = criterion_desc.split()

            # ì§€ì›ìì˜ ëª¨ë“  í† í°ì´ ê¸°ì¤€ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if all(token in criterion_tokens for token in applicant_tokens):
                return criterion.get('score_per_grade', 0)

        return 0

    def _extract_career_info(self, career_content):
        """ê²½ë ¥ ì •ë³´ì—ì„œ ì§ë¬´ëª…ê³¼ ê¸°ê°„ ì¶”ì¶œ"""
        parts = [part.strip() for part in career_content.split(',')]
    
        company = parts[0] if len(parts) > 0 else ""
        job_title = parts[1] if len(parts) > 1 else ""
        duration_text = parts[2] if len(parts) > 2 else ""
    
        # ê¸°ê°„ ì¶”ì¶œ (14ê°œì›” â†’ 14)
        duration_match = re.search(r'(\d+)', duration_text)
        duration_months = int(duration_match.group(1)) if duration_match else 0
    
        return {
            "company": company,
            "job_title": job_title,
            "duration_months": duration_months
        }

    def _calculate_job_similarity(self, applicant_job, target_job):
        """ë‘ ì§ë¬´ëª… ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not applicant_job or not target_job:
            return 0.0
        
        # ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„°í™”
        applicant_embedding = self.embedding_model.encode([applicant_job])
        target_embedding = self.embedding_model.encode([target_job])
    
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(applicant_embedding, target_embedding)[0][0]
        return similarity

    def _evaluate_career_similarity_based(self, rule, applicant_answer_item):
        """ì§ë¬´ ìœ ì‚¬ë„ë¥¼ ê³ ë ¤í•œ ê²½ë ¥ í‰ê°€"""
        career_content = applicant_answer_item.get("resumeContent", "").strip()
        if not career_content:
            return 0
    
        career_info = self._extract_career_info(career_content)
    
        # ì§ë¬´ ìœ ì‚¬ë„ ê³„ì‚°
        target_job_role = rule.get("target_job_role", "")
        similarity = self._calculate_job_similarity(
            career_info["job_title"], 
            target_job_role
        )
    
        print(f"ì§ë¬´ ìœ ì‚¬ë„: {career_info['job_title']} vs {target_job_role} = {similarity:.3f}")
    
        # ìœ ì‚¬ë„ ì„ê³„ê°’ í™•ì¸
        similarity_threshold = rule.get("similarity_threshold", 0.7)
        if similarity < similarity_threshold:
            print(f"ìœ ì‚¬ë„ {similarity:.3f} < ì„ê³„ê°’ {similarity_threshold}, ê°ì  ì ìš©")
            return min(8, self._calculate_duration_score(career_info["duration_months"], rule.get("criteria", [])) * 0.5)
    
        # ìœ ì‚¬ ì§ë¬´ì¸ ê²½ìš° ê¸°ê°„ì— ë”°ë¼ ì •ìƒ ì ìˆ˜ ë¶€ì—¬
        duration_score = self._calculate_duration_score(career_info["duration_months"], rule.get("criteria", []))
        print(f"ìœ ì‚¬ì§ë¬´ í™•ì¸, ê¸°ê°„ {career_info['duration_months']}ê°œì›”ë¡œ {duration_score}ì  ë¶€ì—¬")
    
        return duration_score

    def _calculate_duration_score(self, duration_months, criteria):
        """ê¸°ê°„ì— ë”°ë¥¸ ì ìˆ˜ ê³„ì‚°"""
        sorted_criteria = sorted(criteria, key=lambda x: int(x.get("description", "0")), reverse=True)
        for criterion in sorted_criteria:
            if duration_months >= int(criterion.get("description", "0")):
                return criterion.get("score_per_grade", 0)
        return 0

    def evaluate(self, applicant_data):
        """[ì˜¤ë¥˜ ìˆ˜ì •] Nameì„ keyë¡œ ì‚¬ìš©í•˜ì—¬ ì ìˆ˜ë¥¼ ë°˜í™˜"""
        results_by_name = {}
        
        # ë””ë²„ê¹…: ì…ë ¥ ë°ì´í„° í™•ì¸
        print(f"[DEBUG] QuantitativeEvaluator ì…ë ¥ ë°ì´í„°:")
        print(f"  - resumeItemAnswers ìˆ˜: {len(applicant_data.get('resumeItemAnswers', []))}")
        for item in applicant_data.get('resumeItemAnswers', []):
            print(f"    * {item.get('resumeItemName')} (ID: {item.get('resumeItemId')})")
        
        print(f"  - rules_data resume_items ìˆ˜: {len(self.rules_data.get('resume_items', []))}")
        for rule in self.rules_data.get('resume_items', []):
            print(f"    * {rule.get('name')} (íƒ€ì…: {rule.get('type')})")
        
        for rule in self.rules_data.get('resume_items', []):
            rule_name, item_type = rule['name'], rule['type']
            answer_item = self._find_answer_item(rule_name, applicant_data)
            score = 0
            
            print(f"[DEBUG] ê·œì¹™ '{rule_name}' ì²˜ë¦¬:")
            print(f"  - ë§¤ì¹­ëœ ë‹µë³€: {answer_item}")
            
            if answer_item:
                if item_type == 'CATEGORY': 
                    score = self._evaluate_category(rule, answer_item)
                elif item_type == 'NUMERIC_RANGE': 
                    score = self._evaluate_numeric_range(rule, answer_item)
                elif item_type == 'HOURS_RANGE': 
                    score = self._evaluate_hours_range(rule, answer_item)
                elif item_type == 'DURATION_BASED': 
                    score = self._evaluate_duration_based(rule, answer_item)
                elif item_type == 'RULE_BASED_COUNT': 
                    score = self._evaluate_rule_based_count(rule, answer_item)
                elif item_type == 'SCORE_RANGE': 
                    score = self._evaluate_score_range(rule, answer_item)
                elif item_type in ['TEXT', 'NUMBER', 'DATE', 'FILE', 'SELECT']:
                    # ê¸°ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ í‰ê°€ (ë‚´ìš© ë§¤ì¹­)
                    score = self._evaluate_text_based(rule, answer_item)
                else:
                    print(f"  - ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…: {item_type} (ì ìˆ˜: 0)")
                    score = 0
                print(f"  - ê³„ì‚°ëœ ì ìˆ˜: {score}")
            else:
                print(f"  - ë§¤ì¹­ëœ ë‹µë³€ ì—†ìŒ (ì ìˆ˜: 0)")
                
            results_by_name[rule_name] = score
        
        print(f"[DEBUG] ìµœì¢… ì •ëŸ‰ í‰ê°€ ê²°ê³¼: {results_by_name}")
        return results_by_name


class QualitativeEvaluator:
    """RAGì™€ LLMì„ ì‚¬ìš©í•˜ì—¬ ì •ì„± í‰ê°€ ë° ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ì—”ì§„"""
    def __init__(self, rag_data_path, db_path, collection_name, llm_manager):
        self.rag_criteria = self._load_rag_criteria(rag_data_path)
        self.llm_manager = llm_manager
        self.db_client = chromadb.PersistentClient(path=db_path)
        self.collection = self.db_client.get_collection(
            name=collection_name,
            embedding_function=chromadb.utils.embedding_functions.SentenceTransformerEmbeddingFunction(model_name=settings.EMBEDDING_MODEL)
        )

    def _map_question_id(self, cover_letter_question_id):
        """
        Spring Bootì˜ coverLetterQuestionIdë¥¼ ChromaDBì˜ question_id(1,2)ë¡œ ë§¤í•‘
        P1ê³¼ ë™ì¼í•œ ë§¤í•‘ ë¡œì§ ì‚¬ìš©
        """
        mapped_id = cover_letter_question_id % 2
        if mapped_id == 0:
            mapped_id = 2
        return mapped_id

    def _load_rag_criteria(self, rag_data_path):
        all_rag_data = load_json_file(rag_data_path)
        criteria = {}
        
        # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        print(f"[DEBUG] RAG ê¸°ì¤€ ë¡œë”© ì‹œì‘: {rag_data_path}")
        if not all_rag_data:
            print(f"[ERROR] RAG ë°ì´í„° íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ë¡œë“œ ì‹¤íŒ¨: {rag_data_path}")
            return {}
        
        print(f"[DEBUG] RAG ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(all_rag_data)}ê°œ í•­ëª©")
        
        criterion_count = 0
        for i, item in enumerate(all_rag_data):
            print(f"[DEBUG] Item {i}: type={item.get('type')}, question_id={item.get('question_id')}")
            
            # criterion íƒ€ì…ì¸ ê²½ìš°ë§Œ ì²˜ë¦¬ (example_sentenceëŠ” ì œì™¸)
            if item.get("type") == "criterion":
                q_id = item["question_id"]
                criterion_name = item['content']['name']
                packet_id = item.get("packet_id", f"Q{q_id}_{criterion_name}")
                print(f"[DEBUG] ê¸°ì¤€ ë°œê²¬ (criterion): Q{q_id} - {criterion_name} (packet_id: {packet_id})")
                
                if q_id not in criteria:
                    criteria[q_id] = []
                
                # packet_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬ (ë” ì •í™•í•œ ì¤‘ë³µ ì œê±°)
                existing_packet_ids = [c.get('id', '') for c in criteria[q_id]]
                if packet_id not in existing_packet_ids:
                    # packet_idë¥¼ idë¡œ ì¶”ê°€
                    criterion_content = item["content"].copy()
                    criterion_content["id"] = packet_id
                    criteria[q_id].append(criterion_content)
                    criterion_count += 1
                    print(f"[DEBUG] í‰ê°€ ê¸°ì¤€ ì¶”ê°€: Q{q_id} - {criterion_name} (ì´ {criterion_count}ê°œ)")
                else:
                    print(f"[DEBUG] ì¤‘ë³µ ê¸°ì¤€ ê±´ë„ˆëœ€: Q{q_id} - {criterion_name} (packet_id: {packet_id})")
        
        print(f"[DEBUG] ìµœì¢… criteria êµ¬ì¡°:")
        for q_id, criteria_list in criteria.items():
            print(f"  Q{q_id}: {len(criteria_list)}ê°œ ê¸°ì¤€")
            for criterion in criteria_list:
                print(f"    - {criterion.get('name', 'Unknown')}")
        
        print(f"[DEBUG] ì´ {len(criteria)}ê°œì˜ í‰ê°€ ê¸°ì¤€ ë¡œë“œ ì™„ë£Œ")
        print(f"[DEBUG] question_idë³„ ê¸°ì¤€ ìˆ˜: {[(q_id, len(criteria[q_id])) for q_id in criteria.keys()]}")
        
        return criteria

    def _search_examples(self, question_id, applicant_answer):
        results = self.collection.query(
            query_texts=[applicant_answer], n_results=3,
            where={"question_id": question_id}
        )
        return results['documents'][0] if (results and results['documents']) else []

    def _create_prompt(self, prompt_type, **kwargs):
        if prompt_type == "item_evaluation":
            excellent_desc = next((d['description'] for d in kwargs['criterion']['details'] if d['grade'] == 'EXCELLENT'), "N/A")
            return f"""### ì§€ì‹œ:
ë‹¹ì‹ ì€ ì‹ ì… BE ê°œë°œì ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. [ì§€ì›ì ë‹µë³€]ì„ [í‰ê°€ ê¸°ì¤€]ê³¼ [ìœ ì‚¬ ì˜ˆì‹œ]ë¥¼ ì°¸ê³ í•˜ì—¬ í‰ê°€í•˜ê³ , ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

### í‰ê°€ ëŒ€ìƒ:
[ì§€ì›ì ë‹µë³€]
{kwargs['applicant_answer']}

### í‰ê°€ ê¸°ì¤€: {kwargs['criterion']['name']}
- **ìµœê³ ì (EXCELLENT)**: {excellent_desc}

### ì°¸ê³  ìë£Œ:
[ìœ ì‚¬ ì˜ˆì‹œ]
{kwargs['examples_text']}

### ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ JSON í˜•ì‹ ì¤€ìˆ˜, ë‹¤ë¥¸ ì„¤ëª… ì ˆëŒ€ ì¶”ê°€ ê¸ˆì§€):
{{
  "evaluatedContent": "[ì§€ì›ì ë‹µë³€]ì—ì„œ [í‰ê°€ ê¸°ì¤€]ê³¼ ê°€ì¥ ê´€ë ¨ ê¹Šì€ í•µì‹¬ ë¬¸ì¥ 1ê°œë¥¼ ê·¸ëŒ€ë¡œ ì¶”ì¶œ. ì—†ë‹¤ë©´ 'ì—†ìŒ'ìœ¼ë¡œ ì‘ë‹µ.",
  "grade": "í‰ê°€ ê²°ê³¼ë¥¼ 'ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½' ì¤‘ í•˜ë‚˜ë¡œ í‰ê°€.",
  "evaluationReason": "í‰ê°€ ê·¼ê±°ë¥¼ [í‰ê°€ ê¸°ì¤€]ê³¼ ì—°ê´€ ì§€ì–´ 1ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ ."
}}

### ì¶œë ¥:
"""
        elif prompt_type == "question_summary":
            return f"""### ì§€ì‹œ:
ë‹¹ì‹ ì€ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ [ë¬¸í•­ ë‹µë³€]ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ [ì¶œë ¥ í˜•ì‹]ì— ë§ì¶° JSONìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

### í‰ê°€ ëŒ€ìƒ:
[ë¬¸í•­ ë‹µë³€]
{kwargs['answer_content']}

### ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ JSON í˜•ì‹ ì¤€ìˆ˜, ë‹¤ë¥¸ ì„¤ëª… ì ˆëŒ€ ì¶”ê°€ ê¸ˆì§€):
{{
  "keywords": ["ë‹µë³€ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ 5ê°œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‘ì„±"],
  "summary": "ë‹µë³€ ë‚´ìš©ì„ 1~2 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½."
}}

### ì¶œë ¥:
"""
        elif prompt_type == "overall_analysis":
            return f"""### ì§€ì‹œ:
ë‹¹ì‹ ì€ ìµœê³  ìˆ˜ì¤€ì˜ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ [ì§€ì›ì ì¢…í•© ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§€ì›ìë¥¼ ë‹¤ê°ë„ë¡œ ë¶„ì„í•˜ê³ , ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

### ë¶„ì„ ëŒ€ìƒ:
[ì§€ì›ì ì¢…í•© ì •ë³´]
{kwargs['total_report']}

### ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ JSON í˜•ì‹ ì¤€ìˆ˜, ë‹¤ë¥¸ ì„¤ëª… ì ˆëŒ€ ì¶”ê°€ ê¸ˆì§€):
{{
  "overallEvaluation": "ì§€ì›ìì— ëŒ€í•œ ì¢…í•©ì ì¸ í‰ê°€ë¥¼ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.",
  "strengths": ["ì§€ì›ìì˜ ê°•ì ì„ 3ê°€ì§€ í•­ëª©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‘ì„±"],
  "improvements": ["ì§€ì›ìì˜ ê°œì„ ì ì„ 2ê°€ì§€ í•­ëª©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‘ì„±"],
  "aiRecommendation": "ì¢…í•©ì ì¸ íŒë‹¨ì— ë”°ë¼ 'í•©ê²© ê¶Œì¥' ë˜ëŠ” 'ì‹ ì¤‘í•œ ê²€í†  í•„ìš”' ë˜ëŠ” 'íƒˆë½ ê¶Œì¥' ì¤‘ í•˜ë‚˜ë¡œ ê²°ë¡ .",
  "aiReliability": "í˜„ì¬ ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ AIì˜ ì‹ ë¢°ë„ë¥¼ 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ì†Œìˆ˜ì  ë‘ ìë¦¬ ìˆ«ìë¡œ í‘œí˜„. (ì˜ˆ: 0.87)"
}}

### ì¶œë ¥:
"""

    def _call_llm(self, prompt):
        raw_response = self.llm_manager.generate(prompt)
        try:
            # JSON ê°ì²´ê°€ ì‹œì‘í•˜ëŠ” ì²« '{'ë¥¼ ì°¾ì•„ ê·¸ ì´í›„ì˜ ëª¨ë“  ë‚´ìš©ì„ íŒŒì‹±
            json_part = raw_response[raw_response.find('{'):]
            return json.loads(json_part)
        except (json.JSONDecodeError, AttributeError):
            return None # íŒŒì‹± ì‹¤íŒ¨ ì‹œ None ë°˜í™˜

    def evaluate(self, applicant_data, quant_results): # [ì˜¤ë¥˜ í•´ê²°] quant_results ì¸ì ì¶”ê°€
        self.llm_manager.load_model()
        cover_letter_evals = []
        answers = applicant_data.get('coverLetterQuestionAnswers', [])
        
        # ë””ë²„ê¹…: ì •ì„± í‰ê°€ ì…ë ¥ ë°ì´í„° í™•ì¸
        print(f"[DEBUG] QualitativeEvaluator ì…ë ¥ ë°ì´í„°:")
        print(f"  - coverLetterQuestionAnswers ìˆ˜: {len(answers)}")
        for answer in answers:
            print(f"    * Q{answer.get('coverLetterQuestionId')}: {answer.get('answerContent', '')[:50]}...")
        
        print(f"  - rag_criteria í‚¤ë“¤: {list(self.rag_criteria.keys())}")
        
        for answer_item in answers:
            original_q_id = answer_item['coverLetterQuestionId']  # ì›ë³¸ ID (35, 36 ë“±)
            mapped_q_id = self._map_question_id(original_q_id)    # ë§¤í•‘ëœ ID (1, 2)
            answer_content = answer_item['answerContent']
            answer_evaluations = []
            
            # ë§¤í•‘ëœ question_idë¡œ ê¸°ì¤€ ì°¾ê¸°
            criteria_for_question = self.rag_criteria.get(mapped_q_id, [])
            
            print(f"[DEBUG] Q{original_q_id} -> Q{mapped_q_id} ì •ì„± í‰ê°€:")
            print(f"  - ì°¾ì€ í‰ê°€ ê¸°ì¤€ ìˆ˜: {len(criteria_for_question)}")
            
            # ë§¤í•‘ëœ question_idë¡œ ìœ ì‚¬ ì˜ˆì‹œ ê²€ìƒ‰
            similar_examples = self._search_examples(mapped_q_id, answer_content)
            examples_text = "\n".join([f"- {ex}" for ex in similar_examples]) if similar_examples else "ì—†ìŒ"
            
            for criterion in tqdm(criteria_for_question, desc=f"  - Q{original_q_id} ì •ì„± í‰ê°€ ì¤‘"):
                prompt = self._create_prompt("item_evaluation", applicant_answer=answer_content, criterion=criterion, examples_text=examples_text)
                eval_result = self._call_llm(prompt)
                if eval_result:
                    eval_result["evaluationCriteriaName"] = criterion.get('name', 'Unknown')
                    answer_evaluations.append(eval_result)
            
            summary_prompt = self._create_prompt("question_summary", answer_content=answer_content)
            summary_result = self._call_llm(summary_prompt) or {"keywords": [], "summary": "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"}
            cover_letter_evals.append({
                "coverLetterQuestionId": original_q_id,  # ì›ë³¸ ID ìœ ì§€
                "keywords": summary_result.get('keywords', []),
                "summary": summary_result.get('summary', ''),
                "answerEvaluations": answer_evaluations
            })
        temp_report = {"ì •ëŸ‰ í‰ê°€": quant_results, "ì •ì„± í‰ê°€": cover_letter_evals}
        overall_prompt = self._create_prompt("overall_analysis", total_report=json.dumps(temp_report, ensure_ascii=False, indent=2))
        overall_analysis = self._call_llm(overall_prompt) or {
            "overallEvaluation": "ë¶„ì„ ì‹¤íŒ¨", "strengths": [], "improvements": [],
            "aiRecommendation": "íŒë‹¨ ë¶ˆê°€", "aiReliability": 0.0
        }
        return cover_letter_evals, overall_analysis


def run_p2_pipeline(applicant_data: dict, llm_manager: LLMManager, similarity_evaluator: SimilarityEvaluator):
    """
    P2 íŒŒì´í”„ë¼ì¸ ì „ì²´ë¥¼ ì‹¤í–‰í•˜ê³  ìµœì¢… í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    print(f"--- ğŸ§  1. ì •ëŸ‰ í‰ê°€ ì‹œì‘: {applicant_data.get('applicantName')} ---")
    quant_evaluator = QuantitativeEvaluator(
        rules_path=settings.SCORING_RULES_FILE,
        universities_kb_path=settings.UNIVERSITIES_KB_FILE,
        certifications_kb_path=settings.CERTIFICATIONS_KB_FILE,
        similarity_evaluator=similarity_evaluator
    )
    quant_scores_by_name = quant_evaluator.evaluate(applicant_data)

    print(f"--- ğŸ–‹ï¸ 2. ì •ì„± í‰ê°€ ë° ì¢…í•© ë¶„ì„ ì‹œì‘ ---")
    qual_evaluator = QualitativeEvaluator(
        rag_data_path=settings.RAG_DATA_FILE,
        db_path=settings.DB_PATH,
        collection_name=settings.COLLECTION_NAME,
        llm_manager=llm_manager # ê³µìœ ëœ LLM ë§¤ë‹ˆì € ì‚¬ìš©
    )
    
    cover_letter_evals, overall_analysis = qual_evaluator.evaluate(
        applicant_data, {"scores_by_name": quant_scores_by_name}
    )

    print(f"--- ğŸ“ 3. ìµœì¢… í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ---")
    resume_evaluations = []
    
    # ë””ë²„ê¹…: ì ìˆ˜ ë§¤í•‘ ê³¼ì • í™•ì¸
    print(f"[DEBUG] ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± - ì ìˆ˜ ë§¤í•‘ ê³¼ì •:")
    print(f"  - quant_scores_by_name: {quant_scores_by_name}")
    
    for answer in applicant_data.get('resumeItemAnswers', []):
        item_name = answer['resumeItemName']
        content = answer.get('resumeContent', '') or answer.get('selectedCategory', '')
        score = quant_scores_by_name.get(item_name, 0)
        
        print(f"  - {item_name}: quant_scores_by_nameì—ì„œ ì ìˆ˜ ì¡°íšŒ -> {score}")
        
        resume_evaluations.append({
            "resumeItemId": answer['resumeItemId'],
            "resumeItemName": item_name,
            "resumeContent": content,
            "score": score
        })
    
    final_report = {
        "applicantId": applicant_data.get('applicantId', 0),  # ê¸°ë³¸ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •
        "applicantName": applicant_data.get('applicantName', ''),
        "applicantEmail": applicant_data.get('applicantEmail', ''),
        "applicationId": applicant_data.get('applicationId', 0),  # ê¸°ë³¸ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •
        "jobPostingId": applicant_data.get('jobPostingId', 0),  # ê¸°ë³¸ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •
        "resumeEvaluations": resume_evaluations,
        "coverLetterQuestionEvaluations": cover_letter_evals,
        "overallAnalysis": overall_analysis
    }
    
    print(final_report)
    
    return final_report